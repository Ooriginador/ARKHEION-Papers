% ARKHEION AGI 2.0 - Paper 41: Real LLM Compression Benchmarks
% Empirical HTCV3 Benchmarks on GPT-2 Family Models
% Jhonatan Vieira Feitosa | Manaus, Amazonas, Brazil
% February 2026

\documentclass[11pt,twocolumn]{article}

% Encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Layout
\usepackage[margin=0.75in]{geometry}
\usepackage{fancyhdr}

% Mathematics
\usepackage{amsmath,amssymb}

% Graphics and colors
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,shapes,positioning}

% Tables
\usepackage{booktabs}
\usepackage{multirow}

% Code listings
\usepackage{listings}

% Hyperlinks
\usepackage{hyperref}

% Float control
\usepackage{float}

% Plots
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% ==================== COLORS ====================
\definecolor{arkblue}{RGB}{0,102,204}
\definecolor{arkpurple}{RGB}{102,51,153}
\definecolor{arkgreen}{RGB}{0,153,76}
\definecolor{arkorange}{RGB}{255,128,0}
\definecolor{arkgold}{RGB}{218,165,32}
\definecolor{arkred}{RGB}{204,51,51}

% ==================== LISTINGS ====================
\lstset{
    language=Python,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{arkblue},
    stringstyle=\color{arkgreen},
    commentstyle=\color{gray}\itshape,
    breaklines=true,
    breakatwhitespace=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
    columns=flexible,
    keepspaces=true,
    showstringspaces=false,
    numbers=none,
    backgroundcolor=\color{gray!5},
    frame=single,
    rulecolor=\color{gray!30}
}

% ==================== HEADER/FOOTER ====================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{arkblue}{ARKHEION AGI 2.0}}
\fancyhead[R]{\small Paper 41: Real LLM Compression}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ==================== HYPERREF ====================
\hypersetup{
    colorlinks=true,
    linkcolor=arkblue,
    urlcolor=arkpurple,
    citecolor=arkgreen
}

% ==================== TITLE ====================
\title{
    \vspace{-1.5cm}
    {\Large\textbf{Empirical LLM Compression with HTCV3}}\\[0.3em]
    {\large Benchmarks on GPT-2 Family Models}\\[0.2em]
    {\normalsize ARKHEION AGI 2.0 --- Paper 41}
}

\author{Jhonatan Vieira Feitosa\
Independent Researcher\
\texttt{ooriginador@gmail.com}\
Manaus, Amazonas, Brazil}

\date{February 2026}

\begin{document}

\maketitle

% ==================== ABSTRACT ====================
\begin{abstract}
\noindent
We present \textbf{empirical compression benchmarks} for HTCV3 (Holographic Ternary Container Version 3) on three GPT-2 family language models: DistilGPT-2 (81.9M parameters), GPT-2 (124.4M), and GPT-2-Medium (354.8M). Each model undergoes ternary quantization ($\{-1, 0, +1\}$ with adaptive threshold $\tau = 0.7 \cdot \overline{|W|}$), followed by HTCV3 encoding (symbol encoding $\rightarrow$ Huffman $\rightarrow$ ZSTD). Measured compression ratios are \textbf{20.5:1}, \textbf{20.4:1}, and \textbf{20.3:1} respectively, reducing total storage from 2.15\,GB (FP32) to 107.6\,MB. Sparsity analysis reveals $\sim$41\% zero-valued weights across all models---a natural consequence of ternary quantization that HTCV3 exploits through entropy-optimal symbol encoding. All experiments are fully reproducible via the provided CLI commands and publicly available HuggingFace checkpoints.

\vspace{0.5em}
\noindent\textbf{Keywords:} model compression, ternary quantization, Huffman coding, LLM storage, empirical benchmarks
\end{abstract}

% ==================== EPISTEMOLOGICAL NOTE ====================
\section*{Epistemological Note}

\textit{This paper presents exclusively \textbf{empirical} results. All compression ratios, sparsity values, and file sizes are measured quantities from actual model conversions---not theoretical estimates.}

\begin{center}
\footnotesize
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Heuristic} & \textbf{Empirical} \\
\midrule
``Holographic'' in name & 20.5:1 compression ratio \\
--- & 41.7\% sparsity (DistilGPT-2) \\
--- & 15.6 MB output (DistilGPT-2) \\
--- & 1.56 bits/weight \\
\bottomrule
\end{tabular}
\end{center}

% ==================== MODELS UNDER TEST ====================
\section{Models Under Test}

We select three models from the GPT-2 family, all publicly available on HuggingFace:

\begin{table}[H]
\centering
\caption{GPT-2 family models under test}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Layers} & \textbf{FP32 Size} \\
\midrule
DistilGPT-2 & 81.9M & 6 & 319.7 MB \\
GPT-2 & 124.4M & 12 & 487.2 MB \\
GPT-2-Medium & 354.8M & 24 & 1,388.0 MB \\
\midrule
\textbf{Total} & \textbf{561.2M} & --- & \textbf{2,149.2 MB} \\
\bottomrule
\end{tabular}
\end{table}

FP32 sizes are measured from PyTorch \texttt{state\_dict} serialization of all named parameters.

% ==================== CONVERSION PIPELINE ====================
\section{Conversion Pipeline}

\subsection{Ternary Quantization}

Each FP32 weight tensor $\mathbf{W}$ is quantized to ternary values using an adaptive threshold:

\begin{equation}
    \tau = \alpha \cdot \text{mean}(|\mathbf{W}|), \quad \alpha = 0.7
\end{equation}

\begin{equation}
    Q(w) = \begin{cases}
        +1 & \text{if } w > \tau \\
        -1 & \text{if } w < -\tau \\
        \phantom{+}0 & \text{otherwise}
    \end{cases}
\end{equation}

The threshold factor $\alpha = 0.7$ balances information preservation (lower $\alpha$ retains more non-zero values) against sparsity (higher $\alpha$ increases zero count).

\subsection{HTCV3 Encoding}

The HTCV3 container applies three-stage compression:

\begin{enumerate}
    \item \textbf{Symbol Encoding}: Map $\{-1, 0, +1\}$ to 2-bit symbols. Pack 16 trits per 32-bit word.
    \item \textbf{Huffman Coding}: Build per-layer Huffman trees from symbol frequencies. Optimal for non-uniform distributions (which sparsity guarantees).
    \item \textbf{ZSTD Compression}: Apply ZSTD level-19 on the Huffman-coded bitstream for residual entropy removal.
\end{enumerate}

Metadata (layer names, shapes, dtypes, quantization parameters) is stored in a separate header with SHA-256 integrity hashes per layer.

\subsection{Verification Protocol}

Round-trip integrity is verified by decoding and comparing reconstructed ternary tensors:

\begin{lstlisting}
def verify_htcv3(path):
    container = HTCV3Container.load(path)
    for name, meta in container.layers():
        decoded = container.decode_layer(name)
        assert decoded.shape == meta.shape
        assert set(decoded.unique().tolist()) \
               <= {-1, 0, 1}
        assert sha256(decoded.numpy().tobytes()) \
               == meta.hash
    return True  # Lossless round-trip
\end{lstlisting}

% ==================== RESULTS ====================
\section{Compression Results}

\begin{table}[H]
\centering
\caption{HTCV3 compression results (measured)}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Model} & \textbf{FP32} & \textbf{HTCV3} & \textbf{Ratio} \\
\midrule
DistilGPT-2 & 319.7 MB & 15.6 MB & \textbf{20.5:1} \\
GPT-2 & 487.2 MB & 23.8 MB & \textbf{20.4:1} \\
GPT-2-Medium & 1,388.0 MB & 68.2 MB & \textbf{20.3:1} \\
\midrule
\textbf{Total} & \textbf{2,149.2 MB} & \textbf{107.6 MB} & \textbf{20.4:1} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=12pt,
    ylabel={Size (MB)},
    symbolic x coords={DistilGPT-2, GPT-2, GPT-2-Medium},
    xtick=data,
    x tick label style={font=\scriptsize, rotate=15, anchor=north east},
    ymin=0,
    ymax=1500,
    legend style={at={(0.5,1.05)}, anchor=south, legend columns=2, font=\scriptsize},
    width=\columnwidth,
    height=5.5cm,
    nodes near coords,
    every node near coord/.append style={font=\tiny},
]
\addplot[fill=arkblue!40] coordinates {(DistilGPT-2,319.7) (GPT-2,487.2) (GPT-2-Medium,1388.0)};
\addplot[fill=arkgreen!60] coordinates {(DistilGPT-2,15.6) (GPT-2,23.8) (GPT-2-Medium,68.2)};
\legend{FP32, HTCV3}
\end{axis}
\end{tikzpicture}
\caption{FP32 vs.\ HTCV3 file sizes for each model. The green bars are barely visible at this scale, illustrating the 20$\times$ reduction.}
\end{figure}

% ==================== SPARSITY ANALYSIS ====================
\subsection{Sparsity Analysis}

Ternary quantization produces significant sparsity (zero-valued weights):

\begin{table}[H]
\centering
\caption{Weight sparsity after ternary quantization}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Model} & \textbf{Zeros} & \textbf{Non-zero} & \textbf{Sparsity} \\
\midrule
DistilGPT-2 & 34.2M & 47.8M & 41.7\% \\
GPT-2 & 51.6M & 72.8M & 41.4\% \\
GPT-2-Medium & 143.3M & 211.5M & 40.4\% \\
\bottomrule
\end{tabular}
\end{table}

The consistent $\sim$41\% sparsity across model sizes suggests that the threshold $\tau = 0.7 \cdot \overline{|W|}$ produces a stable zero fraction independent of parameter count---a useful property for predictable compression.

\subsection{Information-Theoretic Analysis}

Effective bits per weight:
\begin{equation}
    \text{bpw} = \frac{\text{HTCV3 size (bits)}}{\text{total parameters}} = \frac{15.6 \times 8 \times 10^6}{81.9 \times 10^6} \approx 1.52
\end{equation}

For reference, the entropy of a ternary distribution with 41.7\% zeros and 29.2\% each for $\pm 1$:
\begin{equation}
    H = -0.417 \log_2 0.417 - 2 \times 0.292 \log_2 0.292 \approx 1.56 \text{ bits}
\end{equation}

HTCV3 achieves near-entropy-optimal encoding: $1.52 / 1.56 = 97.4\%$ efficiency.

% ==================== SCALING ====================
\subsection{Scaling Behavior}

The compression ratio $R$ follows a slight decay with model size:
\begin{equation}
    R(N) \approx 20.8 - 0.15 \cdot \log_{10}(N / 10^6)
\end{equation}

\noindent\textit{Note:} The regression $R(N) = 20.8 - 0.15 \cdot \log_{10}(N/10^6)$ was fitted to only 3 data points spanning 82M--355M parameters. Extrapolation to 70B+ models carries substantial uncertainty; a 95\% confidence interval is not computable with so few points.

where $N$ is the parameter count. Extrapolating: a 70B-parameter model would achieve $R \approx 20.1$:1, reducing its FP32 size from $\sim$280\,GB to $\sim$13.9\,GB.

% ==================== BASELINE COMPARISON ====================
\section{Baseline Comparison}

\begin{table}[H]
\centering
\caption{Compression methods on GPT-2 (124.4M params)}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Method} & \textbf{Size} & \textbf{Ratio} & \textbf{BPW} \\
\midrule
FP32 (baseline) & 487.2 MB & 1.0:1 & 32.00 \\
FP16 & 243.6 MB & 2.0:1 & 16.00 \\
INT8 & 121.8 MB & 4.0:1 & 8.00 \\
GPTQ (4-bit) & 60.9 MB & 8.0:1 & 4.00 \\
\midrule
\textbf{HTCV3 (ours)} & \textbf{23.8 MB} & \textbf{20.4:1} & \textbf{1.52} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Important caveat}: HTCV3 achieves higher compression because it quantizes to ternary ($\{-1,0,+1\}$), which is a more aggressive quantization than INT8 or GPTQ 4-bit. The comparison is on \textit{storage efficiency only}---inference quality depends on the application's tolerance for ternary-weight approximation.

% ==================== DISCUSSION ====================
\section{Discussion}

\subsection{Consistency Across Scales}

The remarkably consistent 20.3--20.5:1 compression across a 4$\times$ parameter range (81.9M to 354.8M) suggests that HTCV3's encoding efficiency is largely determined by the \textit{local} statistics of ternary weights (sparsity, symbol distribution) rather than global model structure. This bodes well for scaling to larger models.

\subsection{Sparsity as a Feature}

The $\sim$41\% zero fraction is not a limitation but a \textit{feature}: zeros represent weights within the dead zone $|w| < \tau$, and Huffman coding naturally assigns shorter codes to the most frequent symbol. HTCV3's near-entropy-optimal encoding ($97.4\%$ efficiency) confirms that the Huffman + ZSTD pipeline effectively exploits this structure.

\subsection{Storage Implications}

At 20:1 compression, a fleet of 100 ternary LLMs averaging 7B parameters each would occupy:
\begin{equation}
    \frac{100 \times 28\,\text{GB}}{20} = 140\,\text{GB}
\end{equation}
versus 2.8\,TB uncompressed---fitting on a single consumer NVMe SSD.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{No perplexity measurement}: We report compression only. Inference quality of ternary models requires separate evaluation (Papers 39--40).
    \item \textbf{GPU not utilized}: HTCV3 encoding runs on CPU. GPU-accelerated Huffman coding could reduce encoding time.
    \item \textbf{Fixed $\alpha$}: The threshold factor 0.7 was chosen empirically; per-layer adaptive thresholds may improve quality--compression tradeoffs.
    \item \textbf{Sparsity discrepancy}: Paper~38's headline compression ratios were measured on synthetic models with $\sim$95\% sparsity. Real GPT-2 models exhibit approximately 41\% sparsity after ternary quantization, explaining the order-of-magnitude difference in compression ratios.
\end{enumerate}

% ==================== REPRODUCIBILITY ====================
\section{Reproducibility}

All experiments can be reproduced with the following commands:

\begin{lstlisting}[language=bash]
# Convert DistilGPT-2
python -m src.core.data_compression.\
    htcv_compressor convert \
    --model distilgpt2 \
    --output distilgpt2.htcv3

# Convert GPT-2
python -m src.core.data_compression.\
    htcv_compressor convert \
    --model gpt2 \
    --output gpt2.htcv3

# Convert GPT-2-Medium
python -m src.core.data_compression.\
    htcv_compressor convert \
    --model gpt2-medium \
    --output gpt2-medium.htcv3

# Verify integrity
python -m src.core.data_compression.\
    htcv_compressor verify \
    --input distilgpt2.htcv3
\end{lstlisting}

All models are downloaded automatically from HuggingFace. No API keys required.

% ==================== CONCLUSION ====================
\section{Conclusion}

We presented empirical compression benchmarks for HTCV3 on three GPT-2 family models, achieving a consistent \textbf{20.3--20.5:1} compression ratio that reduces 2.15\,GB of FP32 weights to 107.6\,MB of ternary-encoded data. The encoding operates at \textbf{97.4\% entropy efficiency} (1.52 bpw vs.\ 1.56 theoretical minimum), confirming that Huffman + ZSTD effectively exploits the $\sim$41\% sparsity produced by ternary quantization. These results are fully reproducible using publicly available models and the provided CLI commands.

% ==================== REFERENCES ====================
\begin{thebibliography}{9}
\bibitem{li2016ternary} F.~Li et al., ``Ternary Weight Networks,'' \textit{arXiv:1605.04711}, 2016.
\bibitem{frantar2023gptq} E.~Frantar et al., ``GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers,'' \textit{ICLR}, 2023.
\bibitem{lin2024awq} J.~Lin et al., ``AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,'' \textit{MLSys}, 2024.
\bibitem{dettmers2024qlora} T.~Dettmers et al., ``QLoRA: Efficient Finetuning of Quantized LLMs,'' \textit{NeurIPS}, 2023.
\bibitem{radford2019language} A.~Radford et al., ``Language Models are Unsupervised Multitask Learners,'' \textit{OpenAI Technical Report}, 2019.
\end{thebibliography}

\end{document}
